{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2024-07-22T18:23:04-04:00"
    },
    {
      "path": "index.html",
      "title": "Brady Falor's Porfolio",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2024-07-22T18:23:17-04:00"
    },
    {
      "path": "logistic.html",
      "title": "Logistic Regression",
      "description": "Explaining and using Logistic Regression ",
      "author": [],
      "contents": "\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\n\ntrain_df <- read_csv(\"~/STA 631/distill-site/data/churn-bigml-80.csv\")\ntest_df <- read_csv(\"~/STA 631/distill-site/data/churn-bigml-20.csv\")\n\n\n\n\nglimpse(train_df)\n\nRows: 2,666\nColumns: 20\n$ State                    <chr> \"KS\", \"OH\", \"NJ\", \"OH\", \"OK\", \"AL\",…\n$ `Account length`         <dbl> 128, 107, 137, 84, 75, 118, 121, 14…\n$ `Area code`              <dbl> 415, 415, 415, 408, 415, 510, 510, …\n$ `International plan`     <chr> \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Ye…\n$ `Voice mail plan`        <chr> \"Yes\", \"Yes\", \"No\", \"No\", \"No\", \"No…\n$ `Number vmail messages`  <dbl> 25, 26, 0, 0, 0, 0, 24, 0, 37, 0, 0…\n$ `Total day minutes`      <dbl> 265.1, 161.6, 243.4, 299.4, 166.7, …\n$ `Total day calls`        <dbl> 110, 123, 114, 71, 113, 98, 88, 79,…\n$ `Total day charge`       <dbl> 45.07, 27.47, 41.38, 50.90, 28.34, …\n$ `Total eve minutes`      <dbl> 197.4, 195.5, 121.2, 61.9, 148.3, 2…\n$ `Total eve calls`        <dbl> 99, 103, 110, 88, 122, 101, 108, 94…\n$ `Total eve charge`       <dbl> 16.78, 16.62, 10.30, 5.26, 12.61, 1…\n$ `Total night minutes`    <dbl> 244.7, 254.4, 162.6, 196.9, 186.9, …\n$ `Total night calls`      <dbl> 91, 103, 104, 89, 121, 118, 118, 96…\n$ `Total night charge`     <dbl> 11.01, 11.45, 7.32, 8.86, 8.41, 9.1…\n$ `Total intl minutes`     <dbl> 10.0, 13.7, 12.2, 6.6, 10.1, 6.3, 7…\n$ `Total intl calls`       <dbl> 3, 3, 5, 7, 3, 6, 7, 6, 5, 5, 2, 5,…\n$ `Total intl charge`      <dbl> 2.70, 3.70, 3.29, 1.78, 2.73, 1.70,…\n$ `Customer service calls` <dbl> 1, 1, 0, 2, 3, 0, 3, 0, 0, 0, 1, 3,…\n$ Churn                    <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, …\n\n\n\nlibrary(naniar)\n\nvis_miss(train_df)\n\n\n\nThere is no missing data! Excellent.\nNow lets do preprocessing\n\n\ntrain_df <- train_df |>\n  mutate(Churn = if_else(Churn, 1, 0))\n\n# Create the recipe\nrec <- recipe(Churn ~ ., data = train_df) %>%\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) %>% # Dummy encode categorical variables\n  step_normalize(all_numeric_predictors()) %>%              # Standardize continuous variables\n  step_corr(all_numeric_predictors(), threshold = 0.9) %>%  # Remove highly correlated variables\n  step_log(all_outcomes(), offset = 1) %>%                  # Log transform skewed distributions\n  prep()\n\n# Apply the recipe to the data\ntrain_processed <- bake(rec, new_data = train_df)\n\n\n\n\n# Create the logistic regression model using glm\nmodel <- glm(Churn ~ ., data = train_processed, family = binomial())\n\n# View the model summary\nsummary(model)\n\n\nCall:\nglm(formula = Churn ~ ., family = binomial(), data = train_processed)\n\nCoefficients:\n                         Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              -2.79382    0.09792 -28.533  < 2e-16 ***\n`Account length`          0.05143    0.07201   0.714 0.475113    \n`Area code`              -0.01334    0.07213  -0.185 0.853222    \n`Number vmail messages`  -0.32444    0.08345  -3.888 0.000101 ***\n`Total day minutes`       0.62067    0.07492   8.284  < 2e-16 ***\n`Total day calls`         0.05901    0.07185   0.821 0.411503    \n`Total eve minutes`       0.25680    0.07419   3.461 0.000538 ***\n`Total eve calls`        -0.02855    0.07227  -0.395 0.692789    \n`Total night calls`       0.02362    0.07088   0.333 0.738922    \n`Total night charge`      0.12808    0.07307   1.753 0.079617 .  \n`Total intl minutes`      0.22891    0.07387   3.099 0.001942 ** \n`Total intl calls`       -0.25836    0.08145  -3.172 0.001514 ** \n`Customer service calls`  0.61391    0.06559   9.359  < 2e-16 ***\nState_AL                  0.03464    0.13982   0.248 0.804338    \nState_AR                  0.10675    0.11395   0.937 0.348877    \nState_AZ                  0.02390    0.13575   0.176 0.860235    \nState_CA                  0.11050    0.09280   1.191 0.233761    \nState_CO                  0.04665    0.13300   0.351 0.725801    \nState_CT                  0.16082    0.12536   1.283 0.199531    \nState_DC                  0.09819    0.12152   0.808 0.419087    \nState_DE                  0.09319    0.12077   0.772 0.440356    \nState_FL                  0.07177    0.12706   0.565 0.572184    \nState_GA                  0.08713    0.11994   0.726 0.467582    \nState_HI                 -0.07262    0.14729  -0.493 0.621973    \nState_IA                  0.04205    0.12452   0.338 0.735565    \nState_ID                  0.03973    0.13584   0.292 0.769926    \nState_IL                 -0.03203    0.12940  -0.248 0.804520    \nState_IN                  0.04210    0.12961   0.325 0.745319    \nState_KS                  0.11752    0.12000   0.979 0.327403    \nState_KY                  0.07228    0.11690   0.618 0.536351    \nState_LA                  0.06086    0.11908   0.511 0.609322    \nState_MA                  0.13325    0.12277   1.085 0.277774    \nState_MD                  0.13078    0.12529   1.044 0.296574    \nState_ME                  0.16986    0.11592   1.465 0.142845    \nState_MI                  0.17145    0.12303   1.394 0.163440    \nState_MN                  0.16932    0.13519   1.252 0.210400    \nState_MO                  0.05660    0.12958   0.437 0.662250    \nState_MS                  0.17509    0.11466   1.527 0.126757    \nState_MT                  0.20567    0.12004   1.713 0.086639 .  \nState_NC                  0.04264    0.12937   0.330 0.741671    \nState_ND                  0.01748    0.12630   0.138 0.889904    \nState_NE                  0.04835    0.12622   0.383 0.701691    \nState_NH                  0.14275    0.11270   1.267 0.205302    \nState_NJ                  0.17551    0.11457   1.532 0.125552    \nState_NM                  0.04242    0.12620   0.336 0.736801    \nState_NV                  0.15049    0.12598   1.195 0.232268    \nState_NY                  0.16014    0.13367   1.198 0.230898    \nState_OH                  0.11081    0.13458   0.823 0.410279    \nState_OK                  0.07676    0.12483   0.615 0.538608    \nState_OR                  0.05995    0.13582   0.441 0.658956    \nState_PA                  0.12743    0.10454   1.219 0.222878    \nState_RI                 -0.10291    0.14260  -0.722 0.470512    \nState_SC                  0.21395    0.11590   1.846 0.064893 .  \nState_SD                  0.06568    0.12332   0.533 0.594301    \nState_TN                  0.04846    0.11776   0.412 0.680667    \nState_TX                  0.23390    0.11788   1.984 0.047235 *  \nState_UT                  0.13374    0.13117   1.020 0.307928    \nState_VA                 -0.08710    0.15609  -0.558 0.576823    \nState_VT                 -0.01623    0.13621  -0.119 0.905135    \nState_WA                  0.16353    0.11569   1.414 0.157507    \nState_WI                 -0.01535    0.14819  -0.104 0.917483    \nState_WV                  0.04319    0.15935   0.271 0.786345    \nState_WY                  0.04128    0.13784   0.300 0.764556    \n`International plan_Yes`  0.57499    0.05343  10.762  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1265.16  on 2665  degrees of freedom\nResidual deviance:  915.93  on 2602  degrees of freedom\nAIC: 1853.3\n\nNumber of Fisher Scoring iterations: 6\n\n\n\ntest_df <- test_df %>%\n  mutate(Churn = if_else(Churn, 1, 0))\n\n\n# Transform the test data using the same recipe\ntest_processed <- bake(rec, new_data = test_df)\n\n# Make predictions on the test data\n# Make predictions on the test data (excluding the Churn column)\npredictions <- predict(model, newdata = select(test_processed, -Churn), type = \"response\")\n\n# Add predictions and actual Churn back to the test data frame for evaluation\ntest_df <- test_df %>%\n  mutate(predicted_Churn = predictions)\n\n\n\n\nlibrary(caret)\nlibrary(pROC)\nlibrary(MLmetrics)\n\n\n# Binarize predictions using a threshold of 0.5\ntest_df <- test_df %>%\n  mutate(predicted_Churn_binary = if_else(predicted_Churn > 0.5, 1, 0))\n\n# Confusion Matrix\nconf_matrix <- confusionMatrix(as.factor(test_df$predicted_Churn_binary), as.factor(test_df$Churn))\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 565  85\n         1   7  10\n                                          \n               Accuracy : 0.8621          \n                 95% CI : (0.8335, 0.8873)\n    No Information Rate : 0.8576          \n    P-Value [Acc > NIR] : 0.3956          \n                                          \n                  Kappa : 0.1415          \n                                          \n Mcnemar's Test P-Value : 9.923e-16       \n                                          \n            Sensitivity : 0.9878          \n            Specificity : 0.1053          \n         Pos Pred Value : 0.8692          \n         Neg Pred Value : 0.5882          \n             Prevalence : 0.8576          \n         Detection Rate : 0.8471          \n   Detection Prevalence : 0.9745          \n      Balanced Accuracy : 0.5465          \n                                          \n       'Positive' Class : 0               \n                                          \n\n# F1 Score\nf1_score <- F1_Score(test_df$Churn, test_df$predicted_Churn_binary)\nprint(paste(\"F1 Score:\", f1_score))\n\n[1] \"F1 Score: 0.924713584288052\"\n\n# AUC\nroc_obj <- roc(test_df$Churn, test_df$predicted_Churn)\nauc_value <- auc(roc_obj)\nprint(paste(\"AUC:\", auc_value))\n\n[1] \"AUC: 0.802355539197644\"\n\n\n\n\n",
      "last_modified": "2024-07-27T17:17:11-04:00"
    },
    {
      "path": "statlearn.html",
      "title": "Statistical Learning",
      "description": "An explanation of Statistical Learning ",
      "author": [],
      "contents": "\nA Primer on Probability\nThe probability of an event is a measurement of how likely it is to occur, and ranges from 0 to 1.\nA probability of 0 means the event will not occur, and a probability of 1 means the event will surely occur.\nTake a coin flip - the probability of flipping heads is 0.5, this is because out of two possible outcomes heads is one of them: 1/2 = 0.5. Each outcome then has a 50% chance of happening. However a small sample size we may not equal results between heads and tails:\n\n\n\nThough if we repeat this coin flip many more times we will begin to see that the frequency of our observations will match the expected probability of 0.5:\n\n\n\nA coin flip results in a binary outcome where there are only two possible results, and we can more or less be certain that it would be impossible to accurately predict these outcomes. However in the real world, we have binary outcomes everywhere and with the help of probability it is possible to predict these outcomes with high accuracy.\nhere are few examples:\nWill someone buy a product?\nWill a sports team win or lose a game?\nWill a customer decide to stop doing business?\nWhat is Statistical Modeling\nData Source\n\n\n\n",
      "last_modified": "2024-07-27T17:14:23-04:00"
    }
  ],
  "collections": []
}
