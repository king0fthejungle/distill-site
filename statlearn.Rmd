---
title: "Statistical Learning"
description: |
  An explanation of how probability lays the foundation for statistical learning
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
set.seed(1999)
```


# Describe probability as a foundation of statistical modeling, including inference and maximum likelihood estimation

## A Primer on Probability using a coin flip

The probability of an event is a measurement of how likely it is to occur, and ranges from 0 to 1. 

A probability of 0 means the event will not occur, and a probability of 1 means the event will surely occur.

Take a coin flip - the probability of flipping heads is 0.5, this is because out of two possible outcomes heads is one of them: 1/2 = 0.5. Each outcome then has a 50% chance of happening. However a small sample size we may not equal results between heads and tails:

```{r}
number_flips <- 10

# Simulate coin flips (0 for tails, 1 for heads)
coin_flips <- sample(c(0, 1), size = number_flips, replace = TRUE)

coin_flips_factor <- factor(coin_flips, labels = c("Tails", "Heads"))
coin_flips_df <- data.frame(coin_flips_factor)

ggplot(data = coin_flips_df, aes(x = coin_flips_factor, fill = coin_flips_factor)) +
  geom_bar() +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5) +
  ylim(0, 10) +
  labs(title = "Histogram of Coin Flips", x = "Outcome", y = "Frequency") +
  scale_fill_manual(values = c("Heads" = "red", "Tails" = "blue")) +
  theme_minimal() +
  theme(legend.position = "none")
```

Though if we repeat this coin flip many more times we will begin to see that the frequency of our observations will match the expected probability of 0.5:

```{r}
number_flips <- 100000

# Simulate coin flips (0 for tails, 1 for heads)
coin_flips <- sample(c(0, 1), size = number_flips, replace = TRUE)

coin_flips_factor <- factor(coin_flips, labels = c("Tails", "Heads"))
coin_flips_df <- data.frame(coin_flips_factor)

ggplot(data = coin_flips_df, aes(x = coin_flips_factor, fill = coin_flips_factor)) +
  geom_bar() +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5) +
  labs(title = "Histogram of Coin Flips", x = "Outcome", y = "Frequency") +
  scale_fill_manual(values = c("Heads" = "red", "Tails" = "blue")) +
  theme_minimal() +
  theme(legend.position = "none")
```

A coin flip results in a binary outcome where there are only two possible results: heads or tails. While we can be certain it would be impossible to accurately predict these outcome of each individual flip, we can be confident about predicting the aggregate results of many flips. For example if we flip a coin 100,000 times we can easily approximate that about 50,000 will be heads, and about 50,000 will be tails. This an exmaple of the concept commonly shared as "regression to the mean". 

## Probability Distributions 

Lets simulate rolling 6-sided dice 10 times, each time we will calculate the sum of rolling both dice and store this sum away. Then we will count the relative frequencies of each sum and visualize them with a histogram. 

```{r}
n_rolls <- 10
die1 <- sample(1:6, n_rolls, replace = TRUE)  # Roll die 1
die2 <- sample(1:6, n_rolls, replace = TRUE)  # Roll die 2
sums <- die1 + die2  # Sum of both dice

data <- data.frame(Sum = sums)

ggplot(data, aes(x = Sum)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Sums of Two Dice Rolls",
       x = "Sum of Dice",
       y = "Frequency") +
  scale_x_continuous(breaks = seq(2, 12, by = 1)) +
  theme_minimal()
```

Here we can be assured that the results of the dice rolls are quite random, however what if we repeat this experiment but instead roll the dice 10,000 times?

```{r}
n_rolls <- 10000
die1 <- sample(1:6, n_rolls, replace = TRUE)  # Roll die 1
die2 <- sample(1:6, n_rolls, replace = TRUE)  # Roll die 2
sums_10k <- die1 + die2  # Sum of both dice

data2 <- data.frame(Sum = sums_10k)

ggplot(data2, aes(x = Sum)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Sums of Two Dice Rolls",
       x = "Sum of Dice",
       y = "Frequency") +
  scale_x_continuous(breaks = seq(2, 12, by = 1)) +  # Ensure x-axis shows whole numbers
  theme_minimal()
```

Now we see that the dice rolls are in-fact not random and there is a discernable pattern, called the "normal distribution". This normal distribution is result of the rules outlined in the Central Limit Theorem. The Central Limit Theorem states that regardless of the original distribution of a random variable (result of a dice roll) the distribution of the sample mean will approach a normal distribution as the sample size becomes large. 


## Statistical Inference

Now lets assume you are tasked with predicting the sum of a the roll of two dice. After viewing the graph, we can expect that because 7 has the greatest relative frequency, it has the highest probability of occurring.

We can perform a t-test to generate a 95% confidence interval on our sets of sums to confirm this:

```{r}
mean_conf_interval <- t.test(sums)$conf.int

print(paste("95% Confidence Interval for the Mean of Dice Roll Sums:",
            round(mean_conf_interval[1], 2), "to", round(mean_conf_interval[2], 2)))
```

Since 7 is the only whole-number that falls within the 95% confidence interval range, we can make a statistical inference that 7 is the prediction that gives use the best odds of being correct. This is the foundation for which statistical models are built upon. 


## Maximum Likelihood Estimation

The concept of Maximum Likelihood is an idea that the parameters of a statistical model can be estimated. Using the probability density of each data point (in our case a roll of two dice), we can measure how likely it is to be observed with various parameter values (in our case we can focus on the mean).

We are confident that our data follows a normal distribution, thus we can simply take the sample mean, and sample standard deviation to maximize the likelihood functions (which are outside the scope of this document). 


```{r}
mle_mean <- mean(sums)
mle_sd <- sd(sums)

print(paste("MLE Estimate of Mean:", round(mle_mean, 2)))
print(paste("MLE Estimate of Standard Deviation:", round(mle_sd, 2)))
```

Given our maximum likelihood estimate for the mean is about 7, we can again confirm that a prediction of 7 would be prudent. Now we have a simple understand of how probability lays the foundation for statistical modeling, and a more complex example using logistic regression can be found on the Logistic page of this site. 




